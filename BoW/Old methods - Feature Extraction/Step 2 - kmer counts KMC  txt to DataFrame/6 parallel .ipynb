{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing  import Pool\n",
    "\n",
    "from typing import Sequence\n",
    "import itertools\n",
    "import pytest\n",
    "# ! pip install pyspark\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#parallel\n",
    "import psutil\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "# ! pip install multiprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all features given a k-mer length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Kmer size \n",
    "k=6\n",
    "features = []\n",
    "\n",
    "def generate_all_kmers(k: int, alphabet=\"CGAT\") -> Sequence[str]:\n",
    "    \"\"\"generate all polymers of length k\n",
    "    Args:\n",
    "        k: length of the polymer\n",
    "        alphabet: base unit of the kmer, [C, G, A, T] for DNA\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_all_kmers(partial_kmers: Sequence[str]):\n",
    "        if len(partial_kmers[0]) == k:\n",
    "            return partial_kmers\n",
    "        return _generate_all_kmers(\n",
    "            partial_kmers=list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    [partial_kmer + n for n in alphabet]\n",
    "                    for partial_kmer in partial_kmers\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return _generate_all_kmers(alphabet)\n",
    "\n",
    "features = generate_all_kmers(k, alphabet=\"ACGT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)\n",
    "# display?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fileNames = []\n",
    "\n",
    "# for i in range(0, 60000):\n",
    "#     s = 'kmers-' + str(k) +'-seqNb-'+ str(i) + '.txt'\n",
    "#     fileNames.append(s)\n",
    "# # print(fileNames)\n",
    "pd.options.display.max_rows = 5\n",
    "pd.options.display.max_columns = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import kmer count files and transform into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "k=6\n",
    "df = pd.DataFrame(columns=features)\n",
    "display(df)\n",
    "# countsPath = r'D:\\DataSet\\MULTI\\test bow\\raw count'\n",
    "# countsPath = r\"D:\\DataSet\\MULTI\\bow\\\\\" + str(k)+ r\"\\mer\\600\"\n",
    "countsPath = r\"D:\\DataSet\\MULTI\\bow\\6mer\\6\"\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    # i = 0\n",
    "#for i in range(0, 600):\n",
    "    print(\"this is process file, fileaname:\", filename)\n",
    "    sample = pd.read_fwf(filename,sep=\" \", header=None).T\n",
    "    display(\"sample : \", sample)\n",
    "    new_header = sample.iloc[0] #grab the first row for the header\n",
    "    sample = sample[1:] #take the data less the header row\n",
    "    sample.columns = new_header #set the header row as the df header\n",
    "\n",
    "    df= df.append(sample, ignore_index=True)  #APPEND Sample to df DataSet\n",
    "    display(df)\n",
    "    return df\n",
    "def get_files(directory,pattern):\n",
    "    '''\n",
    "    Get the files of a directory\n",
    "    '''\n",
    "    for path in Path(directory).rglob(pattern):\n",
    "        yield path.absolute()\n",
    "# def process_file(filename):\n",
    "#     ''''\n",
    "#     Read an xls file, retun a dataframe   \n",
    "#     ''''\n",
    "#     return pd.read_csv(filename,index_col=None)\n",
    "def pd_wrapper(directory,pattern,processes=-1):\n",
    "    # Decide how many proccesses will be created\n",
    "    sum_size = 0\n",
    "    if processes <=0:\n",
    "        num_cpus = psutil.cpu_count(logical=False)\n",
    "    else:\n",
    "        num_cpus = processes\n",
    "        files = []\n",
    "        # Get files based on pattern and their sum of size\n",
    "        for file in get_files(directory=directory,pattern=pattern):\n",
    "                sum_size =sum_size + os.path.getsize(file)\n",
    "                files.append(file)\n",
    "#         display(files)\n",
    "        print('files:%s,size:%s bytes, procs:%s'%(len(files),sum_size,num_cpus))\n",
    "        # Create the pool\n",
    "        process_pool = Pool(processes=num_cpus)\n",
    "        start = time.time()\n",
    "        # Start processes in the pool\n",
    "        dfs = process_pool.map(process_file, files)\n",
    "        # Concat dataframes to one dataframe\n",
    "        data = pd.concat(dfs, ignore_index=True)\n",
    "        end = time.time()\n",
    "        print('Completed in: %s sec'%(end - start))\n",
    "        print(\"data.type\", type(data))\n",
    "        return data\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    df = pd_wrapper(directory=countsPath,pattern='*.txt',processes=8)\n",
    "    print(df.count)\n",
    "    print(type(df))\n",
    "\n",
    "    end = time.time()\n",
    "    # total time taken\n",
    "    print(f\"Runtime of the program is {end - start} secs\")\n",
    "\n",
    "\n",
    "#     display(sample)\n",
    "display(df)\n",
    "# df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "600 rows sequential Runtime of the program is 956.6224915981293 secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(r\"D:\\DataSet\\MULTI\\bow\\df-k\"+str(k)+\".feather\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f.to_pickle(r\"D:\\DataSet\\MULTI\\100-6mers-DF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge sample into single dataframe  using Dask\n",
    "## Result: This approach will not work as Dask doesn't support inserting series with missing values (unlike pandas, which fills Nan for them).\n",
    "### *ValueError: Length mismatch: Expected axis has 1455 elements, new values have 3072 elements"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "test = dd.read_csv(r'D:\\DataSet\\MULTI\\test bow\\raw count\\kmers-6-seqNb-*.txt', sep=\"\\t\", header=0, assume_missing=True)#, names=features)#, header= 0)# header=[:][0])\n",
    "res=test.compute()\n",
    "display(res)#,res3,res4, res5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose: This code transposes files, prerequisite for parallel approach."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(10):\n",
    "    arr = np.genfromtxt(r\"D:\\DataSet\\MULTI\\test bow\\raw count\\kmers-6-seqNb-\"+str(i)+\".txt\", dtype='U').T\n",
    "    arr\n",
    "    np.savetxt(r\"D:\\DataSet\\MULTI\\test bow\\raw count\\kmers-6-seqNb-\"+str(i)+\".txt\", arr, delimiter=\"\\t\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
